#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∞–π—Ç–µ letobasket.ru
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup

async def analyze_site_links():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–π—Ç–µ letobasket.ru"""
    print("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –°–°–´–õ–û–ö –ù–ê –°–ê–ô–¢–ï LETOBASKET.RU")
    print("=" * 60)
    
    try:
        url = "http://letobasket.ru"
        
        print(f"üåê –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å {url}...")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=30) as response:
                if response.status != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status}")
                    return
                
                html = await response.text()
                print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü–∞—Ä—Å–∏–º HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_links = soup.find_all('a', href=True)
        
        print(f"\nüìä –ù–ê–ô–î–ï–ù–û {len(all_links)} –°–°–´–õ–û–ö –ù–ê –°–¢–†–ê–ù–ò–¶–ï")
        print("=" * 60)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
        game_related_links = []
        vizotek_links = []
        basketball_links = []
        
        for i, link in enumerate(all_links):
            href = link.get('href', '')
            link_text = link.get_text().strip()
            
            if not href or not link_text:
                continue
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏–≥—Ä–∞–º–∏
            if any(keyword in href.lower() or keyword in link_text.lower() 
                   for keyword in ['game', 'match', 'podrobno', '–∏–≥—Ä–∞', '–º–∞—Ç—á', '–ø—Ä–æ—Ç–æ–∫–æ–ª']):
                game_related_links.append({
                    'index': i,
                    'href': href,
                    'text': link_text
                })
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –í–∏–∑–æ—Ç–µ–∫
            if '–≤–∏–∑–æ—Ç–µ–∫' in link_text.lower() or 'vizotek' in link_text.lower():
                vizotek_links.append({
                    'index': i,
                    'href': href,
                    'text': link_text
                })
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å –±–∞—Å–∫–µ—Ç–±–æ–ª—å–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
            if any(keyword in link_text.lower() 
                   for keyword in ['–±–∞—Å–∫–µ—Ç', 'basket', '–ª–∏–≥–∞', 'league', '—Ç—É—Ä–Ω–∏—Ä']):
                basketball_links.append({
                    'index': i,
                    'href': href,
                    'text': link_text
                })
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüèÄ –°–°–´–õ–ö–ò, –°–í–Ø–ó–ê–ù–ù–´–ï –° –ò–ì–†–ê–ú–ò ({len(game_related_links)}):")
        for link in game_related_links[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
            print(f"   {link['index']:3d}. {link['text']} -> {link['href']}")
        
        if len(game_related_links) > 20:
            print(f"   ... –∏ –µ—â–µ {len(game_related_links) - 20} —Å—Å—ã–ª–æ–∫")
        
        print(f"\nüéØ –°–°–´–õ–ö–ò –° –£–ü–û–ú–ò–ù–ê–ù–ò–ï–ú –í–ò–ó–û–¢–ï–ö ({len(vizotek_links)}):")
        for link in vizotek_links:
            print(f"   {link['index']:3d}. {link['text']} -> {link['href']}")
        
        if not vizotek_links:
            print("   ‚ùå –°—Å—ã–ª–∫–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –í–∏–∑–æ—Ç–µ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        print(f"\nüèÜ –ë–ê–°–ö–ï–¢–ë–û–õ–¨–ù–´–ï –°–°–´–õ–ö–ò ({len(basketball_links)}):")
        for link in basketball_links[:15]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 15
            print(f"   {link['index']:3d}. {link['text']} -> {link['href']}")
        
        if len(basketball_links) > 15:
            print(f"   ... –∏ –µ—â–µ {len(basketball_links) - 15} —Å—Å—ã–ª–æ–∫")
        
        # –ò—â–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å—Å—ã–ª–æ–∫
        unique_texts = set()
        for link in all_links:
            text = link.get_text().strip()
            if text and len(text) > 2:
                unique_texts.add(text)
        
        print(f"\nüìã –í–°–ï–ì–û –£–ù–ò–ö–ê–õ–¨–ù–´–• –¢–ï–ö–°–¢–û–í –°–°–´–õ–û–ö: {len(unique_texts)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        print("\nüîç –ü–†–ò–ú–ï–†–´ –¢–ï–ö–°–¢–û–í –°–°–´–õ–û–ö:")
        for i, text in enumerate(sorted(unique_texts)[:30]):
            print(f"   {i+1:2d}. {text}")
        
        if len(unique_texts) > 30:
            print(f"   ... –∏ –µ—â–µ {len(unique_texts) - 30} —Ç–µ–∫—Å—Ç–æ–≤")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")

if __name__ == "__main__":
    asyncio.run(analyze_site_links())
