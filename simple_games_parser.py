#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–∞–π—Ç–µ letobasket.ru
"""

import asyncio
import re
from bs4 import BeautifulSoup
import aiohttp
from datetime import datetime, timezone, timedelta

LETOBASKET_URL = "http://letobasket.ru/"

async def analyze_site_structure():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    async with aiohttp.ClientSession() as session:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Cache-Control': 'no-cache'
        }
        
        try:
            async with session.get(LETOBASKET_URL, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    print("üîç –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–ê–ô–¢–ê")
                    print("=" * 50)
                    
                    # –ò—â–µ–º –≤—Å–µ –¥–∞—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    date_pattern = r'\d{2}\.\d{2}\.\d{4}'
                    dates = re.findall(date_pattern, content)
                    unique_dates = list(set(dates))
                    
                    print(f"üìÖ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç: {len(unique_dates)}")
                    for date in sorted(unique_dates):
                        print(f"   {date}")
                    
                    print()
                    
                    # –ò—â–µ–º –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è PullUP
                    pullup_patterns = [
                        r'pull\s*up',
                        r'PullUP',
                        r'Pull\s*Up',
                        r'Pull\s*Up-–§–∞—Ä–º'
                    ]
                    
                    pullup_matches = []
                    for pattern in pullup_patterns:
                        matches = re.finditer(pattern, content, re.IGNORECASE)
                        for match in matches:
                            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                            start = max(0, match.start() - 100)
                            end = min(len(content), match.end() + 100)
                            context = content[start:end]
                            pullup_matches.append(context.strip())
                    
                    print(f"üèÄ –ù–∞–π–¥–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π PullUP: {len(pullup_matches)}")
                    for i, match in enumerate(pullup_matches[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                        print(f"   {i}. {match}")
                    
                    print()
                    
                    # –ò—â–µ–º –∏–≥—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–∫–æ–º–∞–Ω–¥–∞ vs –∫–æ–º–∞–Ω–¥–∞"
                    vs_patterns = [
                        r'([^-]+?)\s+vs\s+([^-]+?)(?:\s|$)',
                        r'([^-]+?)\s+-\s+([^-]+?)(?:\s|$)',
                        r'([^-]+?)\s+–ø—Ä–æ—Ç–∏–≤\s+([^-]+?)(?:\s|$)',
                    ]
                    
                    all_games = []
                    for pattern in vs_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            team1, team2 = match
                            team1 = team1.strip()
                            team2 = team2.strip()
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                            if len(team1) > 2 and len(team2) > 2:
                                all_games.append((team1, team2))
                    
                    print(f"üéÆ –ù–∞–π–¥–µ–Ω–æ –∏–≥—Ä: {len(all_games)}")
                    for i, (team1, team2) in enumerate(all_games[:15], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 15
                        print(f"   {i}. {team1} vs {team2}")
                    
                    print()
                    
                    # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –¥–∞—Ç–∞–º–∏ –∏ –≤—Ä–µ–º–µ–Ω–µ–º
                    datetime_pattern = r'(\d{2}\.\d{2}\.\d{4})\s+(\d{2}:\d{2})'
                    datetime_matches = re.findall(datetime_pattern, content)
                    
                    print(f"‚è∞ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç —Å –≤—Ä–µ–º–µ–Ω–µ–º: {len(datetime_matches)}")
                    for i, (date, time) in enumerate(datetime_matches[:10], 1):
                        print(f"   {i}. {date} {time}")
                    
                    print()
                    
                    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã
                    soup = BeautifulSoup(content, 'html.parser')
                    tables = soup.find_all('table')
                    
                    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
                    for i, table in enumerate(tables[:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ç–∞–±–ª–∏—Ü
                        rows = table.find_all('tr')
                        print(f"   –¢–∞–±–ª–∏—Ü–∞ {i}: {len(rows)} —Å—Ç—Ä–æ–∫")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
                        for j, row in enumerate(rows[:3], 1):
                            cells = row.find_all(['td', 'th'])
                            row_text = ' | '.join([cell.get_text().strip() for cell in cells])
                            if row_text:
                                print(f"     –°—Ç—Ä–æ–∫–∞ {j}: {row_text}")
                    
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status}")
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(analyze_site_structure())
